{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "from SonicBatt import utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "root_dir = utils.root_dir()\n",
    "study_path = os.path.join(root_dir, 'studies', 'multi-cell_ml')\n",
    "data_path = os.path.join(study_path, 'Raw Data')\n",
    "visualistion_path = os.path.join(study_path, 'Visualisation')\n",
    "ancillary_data_path = os.path.join(study_path, 'Ancillary Data')\n",
    "\n",
    "database = pd.read_excel(os.path.join(data_path, 'database.xlsx'))\n",
    "selected_cells = database.loc[database['discarded'] == 'N', 'cell_id'].to_list()\n",
    "\n",
    "time_step = 2.5e-03 #microseconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Identify Acoustic Peaks\n",
    "parquet_filename = 'signals_peaks_fft.parquet'\n",
    "parquet_filepath = os.path.join(ancillary_data_path, parquet_filename)\n",
    "\n",
    "if not os.path.exists(ancillary_data_path):\n",
    "    os.makedirs(ancillary_data_path)\n",
    "if not os.path.exists(parquet_filepath):\n",
    "    for i, test_id in enumerate(database['test_id']):\n",
    "        print('Working on {}'.format(test_id))\n",
    "        test_dir = os.path.join(data_path, test_id)\n",
    "        temp_df = utils.df_with_peaks(data_path, test_id, passes=50)\n",
    "        if i == 0:\n",
    "            df = temp_df\n",
    "        else:\n",
    "            df = pd.concat([df, temp_df], axis=0, ignore_index=True)\n",
    "\n",
    "    # Filter out:\n",
    "    # - Cells which are not in the selected cells\n",
    "    # - The final part of each cell's dataset, if the cell was simply at rest\n",
    "    filter_cells = df[('cycling','Cell_ID')].isin(selected_cells)\n",
    "    filer_final_rest = df[('cycling','V(V)')].notna()\n",
    "    df_shorter = df.loc[filter_cells & filer_final_rest].reset_index(drop=True)\n",
    "\n",
    "    # Calculate FFT coefficients\n",
    "    frequencies_filename = 'frequencies.txt'\n",
    "    frequencies_filepath = os.path.join(ancillary_data_path, frequencies_filename)\n",
    "\n",
    "    # if not os.path.exists(parquet_filepath):\n",
    "    signals = df_shorter['acoustics'].to_numpy()\n",
    "    crop_ind = int(4000/5) - np.argmax(\n",
    "        np.flip(signals[:,:int(4000/5)], axis=1), axis=1).max()\n",
    "    signals_cropped = signals[:,crop_ind:]\n",
    "\n",
    "    if not os.path.exists(frequencies_filepath):\n",
    "        freqs_1d = np.fft.rfftfreq(\n",
    "            n = signals_cropped.shape[1], d = time_step) #MHz\n",
    "        np.savetxt(frequencies_filepath, freqs_1d)\n",
    "\n",
    "    fft_coeffs = np.fft.rfft(signals_cropped, axis=1)\n",
    "    fft_magns = np.abs(fft_coeffs)\n",
    "    fft_magns[:,0] = 0\n",
    "    n_freqs = 301\n",
    "    fft_headings = [str(i) for i in range(1, n_freqs)]\n",
    "    df_fft_magns = pd.DataFrame(data = fft_magns[:,1:n_freqs], columns = fft_headings)\n",
    "\n",
    "    df_final = pd.concat([\n",
    "        df_shorter['cycling'], df_shorter['acoustics'], df_shorter['peak_heights'],\n",
    "        df_shorter['peak_tofs'], df_fft_magns], axis = 1,\n",
    "        keys = ['cycling', 'acoustics','peak_heights', 'peak_tofs',\n",
    "                'fft_magns'])\n",
    "    # Save the concatenated file\n",
    "    df_final.to_parquet(parquet_filepath)\n",
    "else:\n",
    "    df_final = pd.read_parquet(parquet_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Split the dataset for modelling\n",
    "np.random.seed(42)\n",
    "cells_separated_splits = {\n",
    "    'F1': {'test_cell': 'EG_Ac9', 'val_cell': 'EG_Ac8',\n",
    "           'train': None, 'val': None, 'test': None},\n",
    "    'F2': {'test_cell': 'EG_Ac8', 'val_cell': 'EG_Ac6',\n",
    "           'train': None, 'val': None, 'test': None},    \n",
    "    'F3': {'test_cell': 'EG_Ac6', 'val_cell': 'EG_Ac5',\n",
    "           'train': None, 'val': None, 'test': None},  \n",
    "    'F4': {'test_cell': 'EG_Ac5', 'val_cell': 'EG_Ac4',\n",
    "           'train': None, 'val': None, 'test': None}, \n",
    "    'F5': {'test_cell': 'EG_Ac4', 'val_cell': 'EG_Ac3',\n",
    "           'train': None, 'val': None, 'test': None},\n",
    "    'F6': {'test_cell': 'EG_Ac3', 'val_cell': 'EG_Ac2',\n",
    "           'train': None, 'val': None, 'test': None},\n",
    "    'F7': {'test_cell': 'EG_Ac2', 'val_cell': 'EG_Ac9',\n",
    "           'train': None, 'val': None, 'test': None}, \n",
    "}\n",
    "for fold in cells_separated_splits.keys():\n",
    "    test_cell = cells_separated_splits[fold]['test_cell']\n",
    "    val_cell = cells_separated_splits[fold]['val_cell']\n",
    "    filter_test_cell = df_final[('cycling', 'Cell_ID')] == test_cell\n",
    "    filter_val_cell = df_final[('cycling', 'Cell_ID')] == val_cell\n",
    "    cells_separated_splits[fold]['train'] = (\n",
    "        df_final.loc[~(filter_test_cell | filter_val_cell)]).index.to_numpy()\n",
    "    cells_separated_splits[fold]['test'] = (\n",
    "        df_final.loc[filter_test_cell]).index.to_numpy()\n",
    "    cells_separated_splits[fold]['val'] = (\n",
    "        df_final.loc[filter_val_cell]).index.to_numpy()\n",
    "    # Shuffle\n",
    "    np.random.shuffle( cells_separated_splits[fold]['train'] )\n",
    "    np.random.shuffle( cells_separated_splits[fold]['test'] )\n",
    "    np.random.shuffle( cells_separated_splits[fold]['val'] )\n",
    "    assert(True not in np.isin(\n",
    "        cells_separated_splits[fold]['train'],\n",
    "        cells_separated_splits[fold]['val']))\n",
    "    assert(True not in np.isin(\n",
    "        cells_separated_splits[fold]['train'],\n",
    "        cells_separated_splits[fold]['test']))\n",
    "    assert(True not in np.isin(\n",
    "        cells_separated_splits[fold]['val'],\n",
    "        cells_separated_splits[fold]['test']))\n",
    "    # Convert to lists to save a json\n",
    "    cells_separated_splits[fold]['train'] = cells_separated_splits[fold]['train'].tolist()\n",
    "    cells_separated_splits[fold]['test'] = cells_separated_splits[fold]['test'].tolist()\n",
    "    cells_separated_splits[fold]['val'] = cells_separated_splits[fold]['val'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "np.random.seed(42)\n",
    "cells_together_split = {'train': None, 'val': None, 'test': None}\n",
    "#\n",
    "indices_all = df.index.to_numpy().copy()\n",
    "# !!! Instead of what didn't work --> #df.copy(deep=True).index.to_numpy()\n",
    "#\n",
    "n_train = round(0.6 * len(indices_all))\n",
    "n_val = round(0.2 * len(indices_all))\n",
    "np.random.shuffle(indices_all)\n",
    "cells_together_split['train'] = indices_all[:n_train]\n",
    "cells_together_split['val'] = indices_all[n_train:(n_train+n_val)]\n",
    "cells_together_split['test'] = indices_all[(n_train + n_val):]\n",
    "assert(True not in np.isin(\n",
    "    cells_together_split['train'],\n",
    "    cells_together_split['val']))\n",
    "assert(True not in np.isin(\n",
    "    cells_together_split['train'],\n",
    "    cells_together_split['test']))\n",
    "assert(True not in np.isin(\n",
    "    cells_together_split['val'],\n",
    "    cells_together_split['test']))\n",
    "# Convert to lists to save a json\n",
    "cells_together_split['train'] = cells_together_split['train'].tolist()\n",
    "cells_together_split['test'] = cells_together_split['test'].tolist()\n",
    "cells_together_split['val'] = cells_together_split['val'].tolist()\n",
    "\n",
    "# Save the indices for later use\n",
    "with open(os.path.join(ancillary_data_path,'cells_together_split.json'), 'w') as fp:\n",
    "    json.dump(cells_together_split, fp)\n",
    "with open(os.path.join(ancillary_data_path,'cells_separated_splits.json'), 'w') as fp:\n",
    "    json.dump(cells_separated_splits, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% To load the indices from file:\n",
    "with open(os.path.join(ancillary_data_path,'cells_together_split.json'), 'r') as fp:\n",
    "    cells_together_split = json.load(fp)\n",
    "with open(os.path.join(ancillary_data_path,'cells_separated_splits.json'), 'r') as fp:\n",
    "    cells_separated_splits = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Construct spectrograms\n",
    "# First remove the invariant part of the dataframes (the first 758 datapoints)\n",
    "df = df_final.drop(columns = [('acoustics', str(i)) for i in range(758)])\n",
    "time_step=2.5e-03 # microseconds\n",
    "frame_length = 501\n",
    "frame_step = 5\n",
    "crop_freq = 21\n",
    "\n",
    "def make_spec_dataset(waveforms):\n",
    "    # Initialize a list to store spectrograms\n",
    "    specs_list = []\n",
    "    for i, waveform in enumerate(waveforms):\n",
    "        spectrogram = utils.make_spectrogram(\n",
    "            waveform, frame_length=frame_length, frame_step=frame_step, \n",
    "            crop_freq=crop_freq, pad_end=False)\n",
    "        specs_list.append(spectrogram)\n",
    "        # Print progress every 5000 iterations\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"Processed {i + 1} out of {len(waveforms)} waveforms\")\n",
    "    # Convert the list to a numpy array\n",
    "    specs = np.array(specs_list)    \n",
    "    return specs  \n",
    "\n",
    "all_spectrograms = make_spec_dataset(df.loc[:, 'acoustics'].to_numpy())\n",
    "save_dir = os.path.join(ancillary_data_path, 'spectrograms.npy')\n",
    "np.save(save_dir, all_spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save also th labels separately\n",
    "y_values = df_final[('cycling', 'V(V)')].to_numpy()\n",
    "save_dir = os.path.join(ancillary_data_path, 'V_labels.npy')\n",
    "np.save(save_dir, y_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SonicBatt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
